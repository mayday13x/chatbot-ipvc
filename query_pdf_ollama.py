import os
import pdfplumber
from pdf2image import convert_from_path
import pytesseract
from llama_index.core import Document, VectorStoreIndex, Settings
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.core.node_parser import SentenceSplitter
import warnings
import sys
import io
import contextlib

OCR_LANG = 'por'  # OCR em portugu√™s
PDF_DIR = "pdfs"
CACHE_DIR = "pdf_texts_cache"

warnings.filterwarnings("ignore")

@contextlib.contextmanager
def suprimir_stderr():
    stderr_original = sys.stderr
    sys.stderr = io.StringIO()
    try:
        yield
    finally:
        sys.stderr = stderr_original

def extrair_texto_pdf(caminho_pdf):
    texto_total = ""
    # Tenta com pdfplumber
    try:
        with suprimir_stderr():
            with pdfplumber.open(caminho_pdf) as pdf:
                texto_total = "\n".join(page.extract_text() or "" for page in pdf.pages)
    except Exception:
        texto_total = ""

    # Se n√£o extraiu texto, tenta OCR
    if not texto_total.strip():
        print(f"[OCR] Extrair texto via OCR: {os.path.basename(caminho_pdf)}")
        try:
            paginas = convert_from_path(caminho_pdf)
            for imagem in paginas:
                texto_total += pytesseract.image_to_string(imagem, lang=OCR_LANG) + "\n"
        except Exception as e:
            print(f"[ERRO] OCR falhou para {os.path.basename(caminho_pdf)}: {e}")
    return texto_total

def carregar_documentos():
    os.makedirs(CACHE_DIR, exist_ok=True)
    documentos = []

    for arquivo in os.listdir(PDF_DIR):
        if arquivo.lower().endswith(".pdf"):
            caminho_pdf = os.path.join(PDF_DIR, arquivo)
            caminho_cache = os.path.join(CACHE_DIR, arquivo.replace(".pdf", ".txt"))

            if os.path.exists(caminho_cache):
                print(f"[CACHE] Carregando texto cacheado: {arquivo}")
                with open(caminho_cache, "r", encoding="utf-8") as f:
                    texto = f.read()
            else:
                texto = extrair_texto_pdf(caminho_pdf)
                with open(caminho_cache, "w", encoding="utf-8") as f:
                    f.write(texto)

            documentos.append(Document(text=texto, metadata={"fonte": arquivo}))

    return documentos

# Vari√°veis globais para o √≠ndice e o motor de consulta
_index = None
_query_engine = None

def initialize_chatbot():
    global _index, _query_engine
    if _index is None:
        print("[1] Carregando documentos PDF...")
        documentos = carregar_documentos()
        if not documentos:
            print("Nenhum documento PDF encontrado.")
            return None # Retorna None se n√£o houver documentos para indicar falha na inicializa√ß√£o
        print(f"[INFO] {len(documentos)} documentos carregados.")

        print("[2] Configurando embeddings Ollama...")
        Settings.embed_model = OllamaEmbedding(model_name="nomic-embed-text")

        print("[3] Criando √≠ndice LlamaIndex...")
        parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100)
        _index = VectorStoreIndex.from_documents(documentos, transformations=[parser])

        print("[4] Configurando modelo Ollama...")
        llm = Ollama(
            model="llama3:instruct", 
            temperature=0.1,
            system_prompt="""√âs um assistente especializado do Instituto Polit√©cnico de Viana do Castelo. 
            A tua fun√ß√£o √© analisar TODOS os documentos dispon√≠veis e responder a perguntas de forma completa e precisa.
            IMPORTANTE:
            - SEMPRE procura a informa√ß√£o em TODOS os documentos dispon√≠veis
            - NUNCA digas que a informa√ß√£o n√£o est√° presente sem teres verificado TODOS os documentos
            - Se encontrares a informa√ß√£o em qualquer documento, utiliza-a imediatamente
            - Se n√£o encontrares a informa√ß√£o exata, procura informa√ß√µes relacionadas ou contextuais
            - Responde sempre em portugu√™s de Portugal
            - S√™ proativo e detalhado nas respostas
            - Se a informa√ß√£o estiver presente, NUNCA digas que n√£o a encontraste
            - Se a pergunta n√£o estiver relacionada com o IPVC, responde de forma simp√°tica que s√≥ podes responder a quest√µes relacionadas com o IPVC
            - Explica sempre de forma educada e simp√°tica quando uma pergunta est√° fora do teu √¢mbito de conhecimento"""
        )

        _query_engine = _index.as_query_engine(
            llm=llm,
            similarity_top_k=20,
            response_mode="tree_summarize"
        )
        print("[INFO] Chatbot inicializado e pronto.")
    return _query_engine

def get_chatbot_response(pergunta):
    query_engine = initialize_chatbot()
    if query_engine is None:
        return "Erro: O chatbot n√£o foi inicializado. Nenhum documento PDF encontrado ou ocorreu um problema na configura√ß√£o."

    pergunta_formatada = f"""Analisa TODOS os documentos dispon√≠veis e responde em portugu√™s: {pergunta}
    IMPORTANTE: 
    - Procura a informa√ß√£o em TODOS os documentos
    - Se encontrares a informa√ß√£o, responde imediatamente
    - N√£o digas que n√£o encontraste a informa√ß√£o sem teres verificado todos os documentos
    - S√™ detalhado e preciso na resposta
    - Se a pergunta n√£o estiver relacionada com o IPVC, explica de forma simp√°tica que s√≥ podes responder a quest√µes do IPVC"""
    
    resposta = query_engine.query(pergunta_formatada)
    return str(resposta)

if __name__ == "__main__":
    # Este bloco s√≥ ser√° executado se o script for executado diretamente
    # Mantenho para fins de teste manual
    initialize_chatbot()
    print("\n[5] Pronto para perguntas (digite 'sair' para encerrar).")
    while True:
        pergunta = input("\nSua pergunta: ")
        if pergunta.lower() in ("sair", "exit", "quit"):
            break
        resposta = get_chatbot_response(pergunta)
        print("\nüìù Resposta:")
        print(resposta)